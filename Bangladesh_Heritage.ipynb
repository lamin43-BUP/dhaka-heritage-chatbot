{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1sUJNF2edv1",
        "outputId": "632d7e15-39e7-4deb-947e-fce5752c474c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded data from /content/intent.json.\n",
            "Total unique stemmed words (Vocabulary size): 506\n",
            "Total intents/classes: 209\n",
            "Number of documents/patterns: 1361\n",
            "\n",
            "Starting model training (200 epochs)...\n",
            "Model training complete!\n",
            "Final training accuracy: 0.8376\n",
            "\n",
            "======================================================================\n",
            "ğŸŒŸ WELCOME TO DHAKA HERITAGE CHATBOT ğŸŒŸ\n",
            "======================================================================\n",
            "\n",
            "ğŸ›ï¸  Dhaka's Historical Sites - Places You Can Learn About:\n",
            "â€¢ Lalbagh Fort â€“ Mughal-era fort and palace\n",
            "â€¢ Ahsan Manzil â€“ Pink palace, former residence of the Nawab of Dhaka\n",
            "â€¢ Curzon Hall â€“ Historic building of Dhaka University\n",
            "â€¢ Dhakeshwari Temple â€“ Ancient Hindu temple, national temple of Bangladesh\n",
            "â€¢ Armenian Church of Dhaka â€“ Oldest Christian church in Dhaka\n",
            "â€¢ Old Dhaka City Walls â€“ Remains of historic city fortifications\n",
            "â€¢ Shahid Minar â€“ National monument for Language Movement martyrs\n",
            "\n",
            "======================================================================\n",
            "ğŸ¤– Model trained on 209 intents. Start chatting!\n",
            "ğŸ’¡ Type 'quit' to exit the chat.\n",
            "======================================================================\n",
            "\n",
            "Bot: Curzon Hall is primarily an academic building, but visitors can usually access the exterior and certain public areas. However, access to classrooms and laboratories may be restricted during academic activities.\n",
            "Bot: Shahid Minar is open 24 hours daily as it is an open-air monument within Dhaka University campus. However, the best time to visit is during daylight hours from 6:00 AM to 6:00 PM for clear viewing and photography.\n",
            "Bot: Access to the substantial wall remains at Lalbagh Fort requires purchasing an entry ticket. However, many smaller wall fragments integrated into the urban landscape of Old Dhaka can be viewed freely while exploring the historic neighborhoods.\n",
            "Bot: The fort remains open from 10:00 AM to 5:00 PM from Saturday to Thursday.\n",
            "Bot: The entry fee is very reasonable: Bangladeshi citizens pay 20 Taka, while foreign tourists pay 200 Taka. Students get discounted rates with proper ID cards.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- 1. Data Setup (Loading intents.json from file) ---\n",
        "INTENTS_FILE = '/content/intent.json'\n",
        "\n",
        "try:\n",
        "    # Check if the file exists in the current directory\n",
        "    if not os.path.exists(INTENTS_FILE):\n",
        "        print(f\"Error: The file '{INTENTS_FILE}' was not found.\")\n",
        "        print(\"Please upload your 'Intent.json' file to the Colab session's file panel.\")\n",
        "        raise FileNotFoundError(f\"{INTENTS_FILE} not found.\")\n",
        "\n",
        "    with open(INTENTS_FILE, encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    print(f\"Successfully loaded data from {INTENTS_FILE}.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the data: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Initialize stemmer and download NLTK punkt\n",
        "stemmer = LancasterStemmer()\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK punkt...\")\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Try to find punkt_tab and download if not found\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK punkt_tab...\")\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# --- 2. Preprocessing and Feature Extraction ---\n",
        "\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!', '.', ',', \"'s\", \"'re\", \"'m\"]\n",
        "\n",
        "for intent_data in data['intents']:\n",
        "    tag = intent_data['tag']\n",
        "    classes.append(tag)\n",
        "\n",
        "    for pattern in intent_data['patterns']:\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "        documents.append((w, tag))\n",
        "\n",
        "words = [stemmer.stem(w.lower()) for w in words if w.lower() not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print(f\"Total unique stemmed words (Vocabulary size): {len(words)}\")\n",
        "print(f\"Total intents/classes: {len(classes)}\")\n",
        "print(f\"Number of documents/patterns: {len(documents)}\")\n",
        "\n",
        "# Create training data\n",
        "training = []\n",
        "output = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# Training set, bag of words for each pattern\n",
        "for doc in documents:\n",
        "    bag = []\n",
        "    pattern_words = doc[0]\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(output_row)\n",
        "\n",
        "# Convert training data to numpy arrays\n",
        "train_x = np.array(training)\n",
        "train_y = np.array(output)\n",
        "\n",
        "\n",
        "# --- 3. Model Definition and Training ---\n",
        "\n",
        "# Build the Neural Network Model\n",
        "# Using a 3-layer fully connected network (Deep Neural Network)\n",
        "model = Sequential([\n",
        "    Dense(128, input_shape=(len(train_x[0]),), activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(train_y[0]), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nStarting model training (200 epochs)...\")\n",
        "# Train the model (set verbose=1 to see training progress)\n",
        "history = model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=0)\n",
        "print(\"Model training complete!\")\n",
        "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "\n",
        "\n",
        "# --- 4. Prediction Functions ---\n",
        "\n",
        "def clean_up_sentence(sentence):\n",
        "    \"\"\"Tokenizes and stems the sentence.\"\"\"\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "def bag_of_words(sentence, words, show_details=False):\n",
        "    \"\"\"Creates a bag of words (vector) for the input sentence.\"\"\"\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # Bag of words matrix\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print(f\"found in bag: {w}\")\n",
        "    return np.array(bag)\n",
        "\n",
        "def classify_intent(sentence):\n",
        "    \"\"\"Predicts the intent tag for the sentence.\"\"\"\n",
        "    # Generate predictions from the model\n",
        "    input_data = bag_of_words(sentence, words)\n",
        "    try:\n",
        "        results = model.predict(np.array([input_data]), verbose=0)[0]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "    # Filter out predictions below a threshold\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i, r] for i, r in enumerate(results) if r > ERROR_THRESHOLD]\n",
        "    # Sort by probability in descending order\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    # Return tuple of intent and probability\n",
        "    return return_list\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "    \"\"\"Gets a random response based on the highest-confidence intent.\"\"\"\n",
        "    if not intents_list:\n",
        "        return \"I'm sorry, I don't understand that. Can you rephrase?\"\n",
        "\n",
        "    tag = intents_list[0][0]\n",
        "    list_of_intents = intents_json['intents']\n",
        "\n",
        "    for i in list_of_intents:\n",
        "        if i['tag'] == tag:\n",
        "            result = random.choice(i['responses'])\n",
        "            return result\n",
        "    return \"Something went wrong fetching the response.\"\n",
        "\n",
        "\n",
        "# --- 5. Chat Loop Interface ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸŒŸ WELCOME TO DHAKA HERITAGE CHATBOT ğŸŒŸ\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ›ï¸  Dhaka's Historical Sites - Places You Can Learn About:\")\n",
        "print(\"â€¢ Lalbagh Fort â€“ Mughal-era fort and palace\")\n",
        "print(\"â€¢ Ahsan Manzil â€“ Pink palace, former residence of the Nawab of Dhaka\")\n",
        "print(\"â€¢ Curzon Hall â€“ Historic building of Dhaka University\")\n",
        "print(\"â€¢ Dhakeshwari Temple â€“ Ancient Hindu temple, national temple of Bangladesh\")\n",
        "print(\"â€¢ Armenian Church of Dhaka â€“ Oldest Christian church in Dhaka\")\n",
        "print(\"â€¢ Old Dhaka City Walls â€“ Remains of historic city fortifications\")\n",
        "print(\"â€¢ Shahid Minar â€“ National monument for Language Movement martyrs\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"ğŸ¤– Model trained on {len(classes)} intents. Start chatting!\")\n",
        "print(\"ğŸ’¡ Type 'quit' to exit the chat.\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        # 1. Classify the user input (get intent tag)\n",
        "        intent_results = classify_intent(user_input)\n",
        "\n",
        "        # 2. Get a random response based on the intent\n",
        "        bot_response = get_response(intent_results, data)\n",
        "\n",
        "        print(f\"Bot: {bot_response}\")\n",
        "\n",
        "    except EOFError:\n",
        "        # Handles case where input is closed (common in scripted environments)\n",
        "        print(\"\\nChatBot session ended (EOF).\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        time.sleep(1)\n",
        "        break\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Thank you for using Dhaka Heritage Chatbot! ğŸ‘‹\")\n",
        "print(\"Hope you learned something interesting about Dhaka's rich history!\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}